# Problem-Solution Space Exploration Framework

## Overview

This framework helps product teams systematically explore ambiguous problems and generate multiple solution options before committing to building. Problem-solution space exploration is NOT about finding the quickest answerâ€”it's about finding the RIGHT answer.

## Step 0: Braindump & Explore (Critical!)

**Before starting exploration, braindump:**
- What problem are you trying to solve? Dump everything - don't structure yet.
- What does your product sense tell you? What feels like the real problem?
- What assumptions are you making? List them explicitly.
- What biases might affect your exploration? (Solution bias? Jumping to solutions?)

**Product sense exercise:**
- If you had to explain this problem to a 5-year-old, what would you say?
- What would make you say "we're solving the wrong problem"?
- What would make you say "we're solving the right problem"?

## Core Philosophy

### Exploration is NOT Indecision

Effective exploration should:

- **Enable divergence before convergence** - Explore widely, then focus narrowly
- **Separate problem understanding from solution generation** - Donâ€™t jump to solutions
- **Generate multiple options over single ideas** - â€œWhat else?â€ beats â€œIs this good?â€
- **Build shared understanding over individual brilliance** - Align teams through collaborative discovery

### Understanding the Two Spaces

Every product challenge exists in two distinct spaces:

**PROBLEM SPACE (Understanding)**

- What are the customer needs, pain points, desires?
- What is the real underlying issue?
- What context and constraints exist?
- What opportunities exist to create value?

**SOLUTION SPACE (Creating)**

- How might we address these opportunities?
- What are multiple ways to solve this?
- What can we build, buy, or partner?
- Which solution best balances value and effort?

## Framework Structure

### 1. Header Section

Always include:

- **Disclaimer** - â€œThis exploration is iterative; our understanding will evolveâ€
- **Current phase** - Which diamond/phase are you in?
- **Last updated date**
- **Exploration lead** - Whoâ€™s facilitating?
- **Target outcome** - What are you trying to achieve?

### 2. Double Diamond Model (Process)

**Diamond 1: Discover â†’ Define (Problem Space)**

- Discover: Diverge to explore the problem broadly
- Define: Converge to frame the specific problem

**Diamond 2: Develop â†’ Deliver (Solution Space)**

- Develop: Diverge to generate multiple solutions
- Deliver: Converge to test and implement best solution

### 3. Opportunity Solution Tree (Structure)

**Desired Outcome**

- Clear, measurable business goal

**Opportunity Space**

- Customer needs, pain points, desires
- Multiple opportunities that could drive outcome

**Solution Space**

- Multiple solutions per opportunity
- Avoid solution fixation

**Assumption Tests**

- Experiments to validate which solution works best

### 4. Supporting Frameworks

**Jobs to Be Done (JTBD)**

- Understand what customers are trying to accomplish
- Focus on underlying goals, not stated features

**How Might We (HMW) Questions**

- Reframe problems as opportunities
- Generate solution-neutral prompts

**Five Whys**

- Drill down to root causes
- Donâ€™t solve symptoms

## Framework Selection Guide

### Use Double Diamond When:

- Starting a new product or major feature
- Need cross-functional alignment on process
- Stakeholders need visibility into exploration
- Timeline allows for structured phases (8-12 weeks)

### Use Opportunity Solution Tree When:

- Continuous discovery practice
- Multiple opportunities to evaluate simultaneously
- Need to connect solutions back to outcomes
- Want to maintain big-picture view over time

### Use Both Together When:

- Major strategic initiatives
- OST for continuous discovery, Double Diamond for focused sprints
- OST provides structure, Double Diamond provides process

## Writing Guidelines

### Problem Statements

- âœ… â€œNew users struggle to understand core value within first sessionâ€
- âœ… â€œEnterprise customers require compliance features we donâ€™t offerâ€
- âœ… â€œUsers abandon checkout due to unclear shipping costsâ€
- âŒ â€œWe need better onboardingâ€
- âŒ â€œThe UI is confusingâ€
- âŒ â€œWe should add X featureâ€

### Opportunity Framing

- âœ… â€œEnable users to quickly discover personalized contentâ€
- âœ… â€œHelp teams collaborate without frictionâ€
- âœ… â€œReduce time to first value realizationâ€
- âŒ â€œBuild a recommendation engineâ€
- âŒ â€œAdd team featuresâ€
- âŒ â€œMake it fasterâ€

### How Might We Questions

- âœ… â€œHow might we help new users experience value in under 5 minutes?â€
- âœ… â€œHow might we make pricing transparent before checkout?â€
- âœ… â€œHow might we enable async collaboration?â€
- âŒ â€œShould we build a tutorial?â€
- âŒ â€œCan we add tooltips?â€

## Review Schedule

### Daily Check-ins (During Active Exploration)

- Which phase are we in (discover/define/develop/deliver)?
- What did we learn yesterday?
- What are we exploring today?

### Weekly Synthesis

- Key insights from research/interviews
- Patterns emerging
- Updated opportunity map
- Next areas to explore

### Bi-weekly Milestone Reviews

- End of Discover â†’ transition to Define
- End of Define â†’ transition to Develop
- End of Develop â†’ transition to Deliver
- Decision: Continue, pivot, or stop?

### Post-Delivery Retrospective

- What did we learn?
- How accurate were our assumptions?
- What would we do differently?
- Update team playbook

## Stakeholder Communication

### For Leadership

- Show the exploration process creates confidence, not delay
- Present problem-solution fit evidence
- Highlight options considered and why you chose this path
- Connect to business outcomes

### For Product Teams

- Make exploration collaborative, not PM-driven
- Share all research insights transparently
- Involve team in synthesis and framing
- Co-create solutions together

### For Design Teams

- Balance divergent creativity with convergent practicality
- Use prototypes to explore, not just validate
- Test with real users early and often
- Embrace quick, low-fidelity iteration

### For Engineering Teams

- Involve early in problem definition
- Get technical feasibility input during Develop phase
- Use their questions to challenge assumptions
- Co-design solutions with technical constraints in mind

## Common Challenges and Solutions

### â€œWe donâ€™t have time to exploreâ€”just tell us what to buildâ€

**Response pattern:**
â€œExploration prevents building the wrong thing, which is far more expensive than spending 2-4 weeks understanding the problem first. Would you rather spend 1 month exploring and 2 months building the right thing, or 3 months building the wrong thing?â€

### â€œWe keep discovering new problemsâ€”when do we stop?â€

**Response pattern:**
â€œSet a timebox. Spend 2 weeks in Discover, 1 week in Define, regardless of whether you feel â€˜done.â€™ The goal isnâ€™t perfect understandingâ€”itâ€™s sufficient confidence to move forward. Youâ€™ll continue learning as you build.â€

### â€œWe already know the solutionâ€”why waste time exploring alternatives?â€

**Response pattern:**
â€œCompare-and-contrast decisions are better than yes/no decisions. Even if option A seems obvious, generating options B and C helps us understand WHY A is better and spot potential issues. It usually takes 30 minutes and dramatically improves decisions.â€

### â€œOur stakeholder has a specific solution in mindâ€

**Response pattern:**
â€œGreat! Letâ€™s treat that as one solution option. What opportunity does it address? What are 2-3 other ways we could address that same opportunity? This helps us ensure weâ€™re solving the right problem and havenâ€™t missed a better approach.â€

## Best Practices

### Doâ€™s

- Start with the outcome/goal, not the solution
- Interview actual users, not just stakeholders
- Generate multiple solutions per opportunity (3+ minimum)
- Test assumptions cheaply before building
- Visualize your exploration (trees, maps, diagrams)
- Involve cross-functional team throughout
- Document your reasoning for future reference
- Timebox each phase to prevent analysis paralysis

### Donâ€™ts

- Skip directly from problem to solution
- Conduct research in isolation without team synthesis
- Fall in love with your first idea
- Forget to validate with real users
- Let perfection prevent progress
- Do exploration as a one-time activity
- Assume you understand the problem without research
- Build before testing assumptions

## Exploration Success Metrics

Track these to know if your exploration is effective:

- **Solution fitness** - Does shipped solution move target metrics?
- **Assumption accuracy** - How many validated assumptions vs. invalidated?
- **Option generation** - Are teams considering 3+ solutions per opportunity?
- **Time to learning** - How quickly from problem to validated solution?
- **Team alignment** - Can everyone articulate the problem and why this solution?
- **User satisfaction** - Post-launch NPS or satisfaction scores

-----

# Double Diamond Process Guide

## Disclaimer

The Double Diamond is iterative, not linear. You may cycle back to earlier phases as you learn. The goal is progress, not perfection.

**Current Phase:** [Discover / Define / Develop / Deliver]
**Exploration Started:** [Date]
**Phase Target Completion:** [Date]
**Team:** [Product Manager, Designer, Engineer(s), Others]
**Desired Outcome:** [What weâ€™re trying to achieve]

-----

## ğŸ’ Diamond 1: Problem Space

### Phase 1: DISCOVER (Diverge) - Explore the Problem

**Objective:** Understand the problem space thoroughly without jumping to solutions.

**Mindset:** Be curious, not certain. Explore widely. Question assumptions.

**Duration:** 2-4 weeks (depending on scope)

**Key Activities:**

|Activity |Purpose |Methods |Output |
|--------------------------|-----------------------------------|--------------------------------------------------|-------------------------------|
|**User Research** |Understand customer needs & context|Interviews, shadowing, diary studies |User insights, pain points |
|**Data Analysis** |Quantify the problem |Analytics review, cohort analysis, funnel analysis|Usage patterns, drop-off points|
|**Competitive Analysis** |Understand market landscape |Competitor review, feature comparison |Market gaps, opportunities |
|**Stakeholder Interviews**|Gather internal perspectives |Leadership, sales, support interviews |Business constraints, goals |
|**Journey Mapping** |Visualize current experience |Customer journey maps, service blueprints |Pain point identification |

**Research Methods Toolkit:**

**Qualitative Methods:**

- **User Interviews:** 1-on-1 conversations (30-60 min)
- Tip: Ask about last time they experienced the problem, not hypotheticals
- **Contextual Inquiry:** Observe users in their environment
- Tip: Watch what they do, not just what they say
- **Diary Studies:** Users log experiences over time
- Tip: Good for understanding patterns and context
- **Jobs to Be Done Interviews:** Understand what users are trying to accomplish
- Tip: Focus on the â€œjob,â€ not the product

**Quantitative Methods:**

- **Analytics Review:** Understand current behavior patterns
- **Surveys:** Gather data from larger sample
- **A/B Test Results:** Learn from past experiments
- **Cohort Analysis:** How behavior differs by segment

**Outputs from Discover:**

- [ ] 8-15 user interviews completed
- [ ] Key insights synthesized (3-5 major themes)
- [ ] Journey map or service blueprint created
- [ ] Data analysis completed (usage patterns, pain points)
- [ ] Initial opportunity areas identified (5-10)

**Example: Discover Phase Output**

**Project:** Improving new user activation

**Key Insights from 12 user interviews:**

1. New users unclear what to do first (8/12 mentioned)
1. Core value not experienced in first session (10/12)
1. Setup feels overwhelming (7/12)
1. Users donâ€™t understand why they need account (6/12)
1. Success isnâ€™t celebrated/visible (9/12)

**Quantitative validation:**

- 42% activation rate (industry benchmark: 60%)
- 65% drop-off during setup flow
- Users who complete setup are 3.2x more likely to return

-----

### Phase 2: DEFINE (Converge) - Frame the Problem

**Objective:** Synthesize insights into a clear, actionable problem statement.

**Mindset:** Be decisive, not exhaustive. Converge on focus area.

**Duration:** 1-2 weeks

**Key Activities:**

|Activity |Purpose |Methods |Output |
|-------------------------|-------------------------------|-----------------------------------|------------------------|
|**Insight Synthesis** |Find patterns in research |Affinity mapping, thematic analysis|Key themes, patterns |
|**Problem Framing** |Define the core issue |Five Whys, root cause analysis |Problem statement |
|**Opportunity Selection**|Choose what to focus on |Prioritization matrix, voting |Target opportunity |
|**Success Criteria** |Define what â€œsolvedâ€ looks like|Metric definition, target setting |Success metrics |
|**HMW Questions** |Reframe as opportunities |How Might We workshop |Solution-neutral prompts|

**Synthesis Techniques:**

**Affinity Mapping:**

1. Write each insight on sticky note
1. Group related insights together
1. Label each group (themes)
1. Look for larger patterns across groups

**Five Whys:**

```
Problem: Low activation rate
Why? Users don't complete setup
Why? Setup feels overwhelming
Why? Too many steps required upfront
Why? We ask for information we don't need immediately
Why? We haven't prioritized which data is essential
Root cause: Poor information architecture in onboarding
```

**Root Cause Analysis (Fishbone):**
Explore six categories: People, Process, Tools, Environment, Methods, Materials

**Problem Statement Template:**

```
[User type] needs a way to [need/goal]
because [insight]
which leads to [impact]

Currently, [current state/pain point]
resulting in [negative outcome]

We will know we've succeeded when [measurable outcome]
```

**Example:**

```
New users need a way to experience core product value within their first session
because they don't understand what the product can do for them
which leads to abandonment before value realization.

Currently, 42% of signups never complete setup and 65% drop off during the process
resulting in lost customers and missed revenue.

We will know we've succeeded when activation rate increases from 42% to 60%
and time-to-first-value decreases from 8 minutes to under 5 minutes.
```

**How Might We Questions:**

Generate 10-15 HMW questions from your problem statement:

- âœ… HMW help new users discover core value in under 5 minutes?
- âœ… HMW make setup feel effortless rather than overwhelming?
- âœ… HMW celebrate early wins to build momentum?
- âœ… HMW reduce required information during signup?
- âœ… HMW show value before asking for commitment?

**Outputs from Define:**

- [ ] Clear problem statement documented
- [ ] Success metrics defined
- [ ] 10-15 How Might We questions generated
- [ ] Target opportunity selected
- [ ] Constraints and requirements documented

-----

## ğŸ’ Diamond 2: Solution Space

### Phase 3: DEVELOP (Diverge) - Generate Solutions

**Objective:** Generate multiple diverse solutions for the defined problem.

**Mindset:** Be generative, not evaluative. Defer judgment. Think â€œWhat else?â€

**Duration:** 2-3 weeks

**Key Activities:**

|Activity |Purpose |Methods |Output |
|-----------------------|-----------------------|-------------------------------------|----------------------|
|**Ideation** |Generate many solutions|Brainstorming, Crazy 8s, SCAMPER |20-50 solution ideas |
|**Solution Sketching** |Visualize concepts |Storyboards, wireframes, concept maps|Low-fi prototypes |
|**Feasibility Check** |Technical reality check|Engineering spikes, tech research |Feasibility assessment|
|**Concept Development**|Refine top ideas |Detailed wireframes, user flows |3-5 developed concepts|
|**Assumption Mapping** |Identify risks |Assumption listing, risk assessment |Test plan |

**Ideation Techniques:**

**Crazy 8s:**

1. Take one HMW question
1. Fold paper into 8 sections
1. Set timer for 8 minutes
1. Sketch one idea per section (1 min each)
1. Forces rapid, diverse thinking

**SCAMPER:**

- **S**ubstitute: What can we replace?
- **C**ombine: What can we merge?
- **A**dapt: What can we adjust?
- **M**odify: What can we change?
- **P**ut to other use: New way to use existing?
- **E**liminate: What can we remove?
- **R**everse: What if we did opposite?

**Round-Robin Brainstorming:**

1. Each person writes 3 ideas silently (5 min)
1. Pass papers clockwise
1. Build on previous ideas (5 min)
1. Repeat 3-4 times
1. Generates diverse, built-upon ideas

**Solution Generation Rules:**

During ideation:

- âœ… Quantity over quality
- âœ… Build on othersâ€™ ideas (â€œYes, andâ€¦â€)
- âœ… Encourage wild ideas
- âœ… Defer judgment
- âŒ No critiquing
- âŒ No â€œthat wonâ€™t workâ€
- âŒ No solution fixation

**From Ideas to Concepts:**

1. Generate 20-50 raw ideas
1. Cluster similar ideas (affinity grouping)
1. Identify 8-10 distinct concepts
1. Develop top 3-5 concepts with details
1. Create low-fidelity prototypes

**Concept Development Template:**

For each of top 3-5 solutions:

```
Concept Name: [Brief, memorable name]

Description: [2-3 sentences explaining the solution]

Key Features:
- Feature 1
- Feature 2
- Feature 3

How It Addresses the Problem:
[Explain connection to problem statement]

Key Assumptions:
- Assumption 1
- Assumption 2
- Assumption 3

Feasibility Assessment:
- Technical: [Easy / Medium / Hard]
- Design: [Easy / Medium / Hard]
- Effort: [Small / Medium / Large]

Risks:
- Risk 1
- Risk 2
```

**Example: Three Concepts for Activation Problem**

**Concept A: Progressive Setup**

- Show value first, collect info later
- Break setup into micro-steps
- Allow exploration before account creation

**Concept B: Guided First Experience**

- Interactive tutorial with sample data
- Celebrate each completed action
- Personalize based on use case

**Concept C: Social Proof Onboarding**

- Show what others accomplished
- Pre-populated templates to start
- Quick wins in first 2 minutes

**Outputs from Develop:**

- [ ] 20+ solution ideas generated
- [ ] 3-5 detailed solution concepts
- [ ] Low-fidelity prototypes or wireframes
- [ ] Assumptions documented for each concept
- [ ] Feasibility assessments completed

-----

### Phase 4: DELIVER (Converge) - Test & Implement

**Objective:** Validate solutions with users and implement the best option.

**Mindset:** Be evidence-driven, not opinion-driven. Test, learn, iterate.

**Duration:** 2-6 weeks (depending on build complexity)

**Key Activities:**

|Activity |Purpose |Methods |Output |
|----------------------|-----------------------|------------------------------|---------------------------------|
|**Prototype Testing** |Validate with users |Usability tests, concept tests|User feedback, insights |
|**Assumption Testing**|De-risk before building|Smoke tests, concierge tests |Validated/invalidated assumptions|
|**Solution Selection**|Choose best option |Scoring, evidence review |Final solution |
|**Build MVP** |Create minimal version |Agile development |Working MVP |
|**Measure & Learn** |Validate impact |A/B tests, analytics |Impact data |

**Testing Methods:**

**Low-Fidelity Testing:**

- **Paper Prototypes:** Hand-drawn interfaces
- Fast, cheap, great for early feedback
- **Clickable Wireframes:** Basic interactive flows
- Tools: Figma, Balsamiq, Sketch
- **Wizard of Oz:** Human simulates system
- Great for AI/automation features

**Medium-Fidelity Testing:**

- **Interactive Prototypes:** Looks real, limited functionality
- Tools: Figma, Adobe XD, InVision
- **Concierge Testing:** Manually deliver the service
- Validate demand before automating

**High-Fidelity Testing:**

- **Fake Door Test:** Button/page for non-existent feature
- Measures interest before building
- **Limited Beta:** Small user group gets real feature
- Validates in production environment
- **A/B Test:** Compare new vs. old
- Gold standard for validation

**Usability Testing Protocol:**

**Preparation:**

1. Define what you want to learn
1. Create testing script with tasks
1. Recruit 5-8 users per concept
1. Prepare prototype

**During Test (30-45 min):**

1. Intro & context (5 min)
1. Task completion (20-30 min)
- Think aloud protocol
- Observe, donâ€™t help
1. Retrospective questions (5-10 min)
- What worked? What confused?
- Would you use this?

**After Test:**

1. Synthesize findings across users
1. Identify patterns (3+ users = pattern)
1. Rate severity: Critical / Major / Minor
1. Iterate or select solution

**Solution Selection Framework:**

Score each concept on:

|Criterion |Weight|Concept A|Concept B|Concept C|
|----------------------------------------------------------|------|---------|---------|---------|
|**User Desirability** (Does solving the problem matter?) |30% |Score |Score |Score |
|**Solution Usability** (Can users use this effectively?) |25% |Score |Score |Score |
|**Technical Feasibility** (Can we build this?) |20% |Score |Score |Score |
|**Business Viability** (Does this support business goals?)|15% |Score |Score |Score |
|**Time to Value** (How fast can we ship and learn?) |10% |Score |Score |Score |
|**Total Weighted Score** |100% |Total |Total |Total |

Use 1-5 scale for each criterion.

**Build-Measure-Learn Loop:**

1. **Build:** Create minimum viable version
- Include instrumentation
- Ship to small % of users
1. **Measure:** Track key metrics
- Success metrics defined in Define phase
- User behavior data
- Qualitative feedback
1. **Learn:** Analyze results
- Did it move the metric?
- What surprised us?
- What should we change?

**Outputs from Deliver:**

- [ ] 5-8 usability tests completed per concept
- [ ] Key assumptions validated or invalidated
- [ ] Solution selected with evidence
- [ ] MVP built and shipped
- [ ] Impact measured against success criteria
- [ ] Learnings documented for future

-----

# Opportunity Solution Tree Guide

## Disclaimer

The Opportunity Solution Tree is a living document. It evolves as you conduct continuous discovery and learn more about your customers and opportunities.

**Last Updated:** [Date]
**Next Review:** [Date]
**Product Trio:** [PM, Designer, Engineer]
**Interview Cadence:** [Weekly / Bi-weekly]

-----

## ğŸŒ³ Opportunity Solution Tree Structure

```
Desired Outcome
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ â”‚
Opportunity 1 Opportunity 2 Opportunity 3
â”‚ â”‚ â”‚
â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â”
â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
S1 S2 S3 S4 S5 S6 S7 S8 S9
â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
E1 E2 E3 E4 E5 E6 E7 E8 E9

Legend:
Outcome = Desired business/product outcome
Opportunity = Customer need, pain point, desire
S = Solution (multiple per opportunity)
E = Experiment/Assumption Test
```

-----

## ğŸ¯ Step 1: Define Your Desired Outcome

**Purpose:** Establish the North Star for this workâ€”what business and customer value youâ€™re trying to create.

**Characteristics of Good Outcomes:**

- Measurable and specific
- Balances business and customer value
- Within teamâ€™s control to influence
- Time-bound (usually quarterly)
- Motivating and clear

**Outcome Template:**

```
[Verb] [Metric] [by X%] [by date]

Examples:
- Increase trial-to-paid conversion rate by 25% by Q2
- Reduce time-to-first-value from 8 min to 5 min by Q3
- Improve 30-day retention from 45% to 60% by year-end
```

**Bad Outcomes (Outputs, not outcomes):**

- âŒ Ship 5 new features this quarter
- âŒ Improve the onboarding experience
- âŒ Make users happy

**Good Outcomes (Impact-focused):**

- âœ… Increase activation rate from 42% to 60% by Q2
- âœ… Reduce support tickets by 30% by Q3
- âœ… Grow weekly active users by 20% by year-end

**Your Desired Outcome:**

```
Outcome: [Write your specific, measurable outcome]

Why this matters to business: [Business value]

Why this matters to customers: [Customer value]

How we'll measure success: [Specific metrics and targets]

Timeframe: [When we expect to achieve this]
```

-----

## ğŸ” Step 2: Map the Opportunity Space

**Purpose:** Identify customer needs, pain points, and desires that, if addressed, would drive your desired outcome.

**Where Opportunities Come From:**

- User interviews (primary source)
- Support tickets and feedback
- Analytics and data analysis
- Sales conversations
- User research studies

**âš ï¸ Critical Rule:** Opportunities must come primarily from actual customer interviews (target: 1 interview per week minimum).

**Interview Questions to Uncover Opportunities:**

- Tell me about the last time you tried to [accomplish goal]
- What were you trying to achieve?
- What made that difficult?
- How did you work around it?
- What would make that easier?

**Opportunity Mapping Process:**

1. **Conduct 6-8 interviews** to start
1. **Identify customer stories** where they struggled or had needs
1. **Extract opportunities** from stories
1. **Group related opportunities** into categories
1. **Structure hierarchically** (parent opportunities â†’ sub-opportunities)

**Opportunity Characteristics:**

**Good Opportunities:**

- âœ… Expressed in customer language
- âœ… Focused on needs, not solutions
- âœ… Clearly connected to outcome
- âœ… Multiple people experience it
- âœ… Specific and concrete

**Bad Opportunities (Actually solutions):**

- âŒ â€œNeed a better UIâ€
- âŒ â€œWant AI recommendationsâ€
- âŒ â€œNeed faster performanceâ€

**Opportunity Template:**

```
Opportunity: [Customer need/pain point/desire in their words]

Customer Evidence:
- Interview quote 1: "[Quote from customer]" - [Customer ID/Name]
- Interview quote 2: "[Quote from customer]" - [Customer ID/Name]
- Data point: [Quantitative evidence]

Connection to Outcome: [How addressing this would drive the outcome]

Scope:
- How many customers affected: [Estimate]
- How often they experience this: [Frequency]
- Current satisfaction level: [1-5 scale]

Priority: [High / Medium / Low] based on [Criteria]
```

**Example Opportunity Map:**

```
Outcome: Increase activation rate from 42% to 60%

Opportunities (from 12 user interviews):

1. QUICKLY UNDERSTAND WHAT THE PRODUCT DOES
- See relevant examples for my use case
- Know if this will solve my problem
- Understand value before committing time

2. EXPERIENCE VALUE WITHOUT SETUP FRICTION
- Try core features immediately
- Not blocked by account creation
- Skip unnecessary information collection

3. FEEL CONFIDENT I'M DOING IT RIGHT
- Know if I'm on the right track
- Understand next steps
- Get validation of progress

4. CELEBRATE EARLY WINS
- See tangible results quickly
- Feel sense of achievement
- Build momentum to continue
```

**Opportunity Evaluation Criteria:**

Before selecting which opportunity to focus on, evaluate using:

**Opportunity Sizing:**

- How many customers affected?
- How often do they experience this?

**Market Factors:**

- How does this position us competitively?
- Is this a differentiator?

**Company Factors:**

- Alignment with company strategy?
- Resources available?

**Customer Factors:**

- How important to customers?
- Current satisfaction level?

-----

## ğŸ’¡ Step 3: Generate Multiple Solutions

**Purpose:** For your target opportunity, generate 3+ different ways to address it.

**Critical Rule:** Resist falling in love with your first idea. Always ask â€œWhat else could we build?â€

**Solution Generation Principles:**

- Generate 3-5 solutions per opportunity minimum
- Make solutions diverse (not minor variations)
- Keep solution descriptions brief (1-2 sentences)
- Donâ€™t commit to building yetâ€”just exploring options

**Solution Template:**

```
Solution Name: [Brief, memorable name]

Description: [1-2 sentence explanation]

Key Assumption: [What needs to be true for this to work?]

Effort Estimate: [Small / Medium / Large]
```

**Example: Solutions for â€œExperience Value Without Setup Frictionâ€**

**Solution 1: Guest Mode**

- Allow full product exploration without account
- Create account only when saving work
- Assumption: Users will create account after experiencing value

**Solution 2: Social Login + Smart Defaults**

- One-click signup with Google/LinkedIn
- Pre-populate profile from social data
- Assumption: Reducing clicks removes friction

**Solution 3: Progressive Information Collection**

- Collect info just-in-time, not upfront
- Each feature asks for what it needs
- Assumption: Spreading requests feels less overwhelming

**Solution 4: Sample Project Pre-loaded**

- New users start with example project
- Can explore and modify immediately
- Assumption: Hands-on learning > empty state

**Solution 5: Wizard with Skip Options**

- Guided setup with ability to skip any step
- Return to skipped items later
- Assumption: Flexibility reduces abandonment

**Compare & Contrast Questions:**

- Which solution addresses the opportunity most directly?
- Which has the lowest risk/highest confidence?
- Which can we test fastest/cheapest?
- Which aligns with our product strategy?
- What would we lose by not choosing each option?

-----

## ğŸ§ª Step 4: Design Assumption Tests

**Purpose:** Validate solutions before fully building them.

**Key Principle:** Every solution has assumptions. Test the riskiest assumptions first.

**Types of Assumptions:**

**Desirability:** Do customers want this?
**Viability:** Does this support the business?
**Feasibility:** Can we build this?
**Usability:** Can customers use this?
**Ethical:** Should we build this?

**Assumption Mapping:**

For each solution, list key assumptions:

```
Solution: [Solution name]

Assumptions (in order of risk):
1. [Highest risk assumption]
2. [Medium risk assumption]
3. [Lower risk assumption]

Test Plan:
- Assumption: [What we believe]
- Test Method: [How we'll validate]
- Success Criteria: [What would prove us right]
- Timeline: [When we'll complete]
```

**Example: Assumption Tests for â€œGuest Modeâ€**

```
Solution: Guest Mode

Assumption 1: Users will explore features without account
- Test: Clickable prototype with analytics
- Success: 70%+ complete a core task in guest mode
- Timeline: 1 week

Assumption 2: Users will create account after experiencing value
- Test: Fake door test "Save your work"
- Success: 40%+ click "Create account" to save
- Timeline: 1 week

Assumption 3: Guest-to-paid conversion is comparable to regular signup
- Test: Build lightweight version, measure conversion
- Success: Conversion rate within 20% of regular flow
- Timeline: 3 weeks
```

**Test Method Options:**

**Cheap & Fast (Days):**

- Interviews with prototype
- Fake door test
- Landing page test
- Survey

**Medium (1-2 weeks):**

- Clickable prototype with users
- Wizard of Oz (manual backend)
- Concierge test
- Limited beta

**Expensive & Slow (2-6 weeks):**

- Full build with A/B test
- Pilot program
- Beta release

**Test Prioritization:**

- Test highest-risk assumptions first
- Test multiple solutions in parallel when possible
- Stop testing if assumptions invalidated
- Donâ€™t over-testâ€”build when confidence is sufficient

-----

## ğŸ”„ Continuous Discovery Process

**Weekly Cadence:**

**Monday/Tuesday:**

- Conduct 1-2 user interviews
- Focus on target opportunity area
- Record and note-take

**Wednesday:**

- Synthesize interview insights
- Update Opportunity Solution Tree
- Identify new opportunities or validate existing

**Thursday:**

- Review assumption test results
- Update solution options based on learning
- Plan next tests

**Friday:**

- Team synthesis session
- Decide which solutions to test next
- Update stakeholders on progress

-----

## ğŸ“Š Opportunity Solution Tree Template

### Visual Map

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ OUTCOME: [Your measurable outcome] â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ â”‚ â”‚ â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚ Opp 1 â”‚ â”‚ Opp 2 â”‚ â”‚ Opp 3 â”‚ â”‚ Opp 4 â”‚
â”‚ [Name] â”‚ â”‚ [Name] â”‚ â”‚ [Name] â”‚ â”‚ [Name] â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
â”‚ â”‚ â”‚ â”‚
â”Œâ”€â”€â”€â”¼â”€â”€â”€â”¬â”€â”€â”€â”€â” â”‚ â”‚ â”‚
â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
S1 S2 S3 S4 S5 S6 S7
â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚
E1 E2 E3 E4 E5 E6 E7
```

### Detailed Documentation

**OUTCOME**

```
Metric: [Specific metric you're trying to move]
Current State: [Baseline value]
Target State: [Goal value]
Timeframe: [When]
Why This Matters: [Business + customer value]
```

-----

**OPPORTUNITY 1: [Name in customer language]**

```
Description: [What customers are trying to accomplish/problem they face]

Evidence:
- Customer quote 1: "[Quote]" - [Source]
- Customer quote 2: "[Quote]" - [Source]
- Data: [Quantitative evidence]

Impact Assessment:
- Frequency: [How often experienced]
- Scope: [How many customers]
- Severity: [1-5 scale]
- Current satisfaction: [1-5 scale]

Connection to Outcome: [How this drives the metric]

Status: [Exploring / Testing / Shipped / Deprioritized]
```

**Solution 1A: [Name]**

- Description: [1-2 sentences]
- Key Assumption: [What must be true]
- Effort: [S/M/L]
- Status: [Idea / Testing / Building / Shipped]

**Experiment 1A-1:**

- Assumption tested: [Specific belief]
- Method: [How you tested]
- Success criteria: [What you measured]
- Result: [Validated / Invalidated / Inconclusive]
- Learning: [What you learned]
- Next step: [What youâ€™ll do based on this]

**Solution 1B: [Name]**

- [Same structure]

**Solution 1C: [Name]**

- [Same structure]

-----

**OPPORTUNITY 2: [Name]**
[Repeat structure]

-----

## ğŸ”„ Tree Maintenance

### Monthly Tree Review

**Questions to Ask:**

**About Outcome:**

- [ ] Is this outcome still the right focus?
- [ ] Have we made progress? (Show data)
- [ ] Do we need to adjust target or timeframe?

**About Opportunities:**

- [ ] Are these still the right opportunities?
- [ ] Have we discovered new opportunities?
- [ ] Should we reprioritize based on learning?
- [ ] Are any opportunities validated/invalidated?

**About Solutions:**

- [ ] Do we have 3+ solutions per opportunity?
- [ ] Are solutions diverse enough?
- [ ] Have we tested enough assumptions?
- [ ] What should we build next?

**Tree Health Indicators:**

âœ… **Healthy Tree:**

- Weekly customer interviews happening
- Multiple opportunities identified (5-10)
- 3+ solutions per target opportunity
- Active assumption testing
- Clear evidence trail
- Regular updates based on learning

âš ï¸ **Unhealthy Tree:**

- No recent customer interviews
- Single opportunity or solution fixation
- No experiments running
- Tree hasnâ€™t been updated in weeks
- Missing customer evidence
- Building without validating

-----

## ğŸ¯ Combining Double Diamond + OST

### Use Together for Maximum Impact

**Opportunity Solution Tree = Continuous Practice**

- Ongoing customer discovery
- Always exploring opportunity space
- Multiple solutions always ready

**Double Diamond = Focused Sprint**

- When you need to converge and ship
- 6-12 week intensive exploration
- Deep dive on specific opportunity

### Integration Model

```
OST (Continuous)
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–º
â”‚ â”‚
â–¼ â–¼
DD Sprint 1 DD Sprint 2
(8 weeks) (8 weeks)

â”‚â†Discoverâ†’â”‚
â”‚â†Defineâ†’â”‚
â”‚â†Developâ†’â”‚
â”‚â†Deliverâ†’â”‚
```

**How They Work Together:**

**OST Feeds Double Diamond:**

- Use OST opportunities as DD starting point
- OST interviews provide research foundation
- OST solutions become DD ideation inputs

**Double Diamond Feeds OST:**

- DD learnings update OST
- Shipped solutions inform new opportunities
- DD experiments become OST evidence

**Example Workflow:**

**Weeks 1-12: Continuous OST**

- Weekly interviews
- Building opportunity map
- Generating multiple solutions
- Running small tests

**Weeks 13-20: Double Diamond Sprint**

- **Discover (2 weeks):** Deep dive on top opportunity from OST
- **Define (1 week):** Synthesize into clear problem
- **Develop (2 weeks):** Generate and prototype solutions
- **Deliver (3 weeks):** Test, build, ship

**Weeks 21+: Back to OST**

- Update tree with DD learnings
- Measure impact of shipped solution
- Explore next opportunity

-----

## ğŸ› ï¸ Supporting Frameworks

### Jobs to Be Done (JTBD)

**When to Use:** Understanding the underlying â€œjobâ€ customers are hiring your product to do.

**Core Concept:** Customers donâ€™t buy products; they hire them to make progress in their lives.

**JTBD Interview Format:**

```
Timeline Questions:
1. When did you first realize you needed something like this?
2. What were you doing at that moment?
3. What made that the right time to look for a solution?
4. What did you try before finding our product?
5. What made you choose us over alternatives?
6. What happened after you started using it?
```

**JTBD Statement Template:**

```
When [situation],
I want to [motivation],
So I can [expected outcome].

Example:
When I'm preparing for an important presentation,
I want to quickly find relevant data insights,
So I can make compelling arguments backed by evidence.
```

**Forces of Progress (JTBD):**

```
Push: Current pain points
â†“
[Current State] â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ [New State]
â†‘ â†‘
Anxiety: Fear of change Pull: Attraction to new solution

Habit: Inertia of current
```

-----

### Five Whys

**When to Use:** Getting to root causes instead of symptoms.

**Process:**

1. State the problem
1. Ask â€œWhy does this happen?â€
1. Ask â€œWhy?â€ to that answer
1. Repeat 5 times (or until you reach root cause)
1. Address the root cause, not symptoms

**Example:**

```
Problem: Users abandon onboarding

Why? They don't complete the setup flow
Why? The setup flow is too long
Why? We ask for too much information upfront
Why? We designed the flow to collect all data at once
Why? We assumed we needed everything before users could start

Root Cause: We prioritized our data needs over user experience
```

**Tips:**

- Sometimes you need more than 5 whys
- Sometimes you reach root cause sooner
- Can branch when multiple causes exist
- Works best with diverse team perspectives

-----

### How Might We (HMW) Questions

**When to Use:** Reframing problems as opportunities.

**Format:** â€œHow might we [action] so that [benefit]?â€

**Characteristics of Good HMWs:**

- Optimistic and possibility-oriented
- Solution-neutral (doesnâ€™t suggest a specific solution)
- Broad enough to generate options
- Narrow enough to be actionable

**Problem â†’ HMW Transformation:**

```
âŒ Problem: "Users are confused by our UI"
âœ… HMW: "How might we help users intuitively navigate to their goals?"

âŒ Problem: "Setup takes too long"
âœ… HMW: "How might we help users experience value within 5 minutes?"

âŒ Problem: "We need better documentation"
âœ… HMW: "How might we enable users to solve problems independently?"
```

**HMW Generation Workshop:**

1. Start with problem statement (15 min)
1. Individually write HMWs (10 min)
1. Share and build on each otherâ€™s (15 min)
1. Group similar HMWs (10 min)
1. Vote on most promising (10 min)
1. Select 3-5 to ideate on (immediately)

-----

## ğŸ“ Quick Decision Guide

### Which Framework Should I Use?

**Use Double Diamond when:**

- âœ… Major product initiative (new product, major feature)
- âœ… Clear start and end needed
- âœ… Cross-functional team needs structure
- âœ… Stakeholders need visibility into process
- âœ… You have 6-12 week dedicated time

**Use Opportunity Solution Tree when:**

- âœ… Continuous discovery practice
- âœ… Multiple opportunities to evaluate
- âœ… Want big-picture view over time
- âœ… Building product trio rhythm
- âœ… Need to show option generation

**Use Jobs to Be Done when:**

- âœ… Understanding motivation and context
- âœ… Exploring switching moments
- âœ… Identifying competition (not just direct competitors)
- âœ… Strategic positioning questions
- âœ… Finding unmet needs in market

**Use Five Whys when:**

- âœ… Symptoms vs. root causes unclear
- âœ… Quick problem diagnosis needed
- âœ… Team has different hypotheses
- âœ… Youâ€™re solving same problem repeatedly
- âœ… 15-30 minute exercise needed

**Use How Might We when:**

- âœ… Need to reframe problems positively
- âœ… Kickstarting ideation sessions
- âœ… Moving from problem to solution space
- âœ… Breaking down large problems
- âœ… Creating generative prompts

-----

## âš ï¸ Common Pitfalls & Solutions

### Pitfall #1: Jumping to Solutions Too Quickly

**What it looks like:**

- First idea becomes â€œthe solutionâ€
- Skipping problem validation
- Building without exploring alternatives

**Why it happens:**

- Pressure to ship fast
- Solution already in mind
- Uncomfortable with ambiguity

**Solution:**

- Force yourself to generate 3+ solutions
- Timebound problem space work (2-3 weeks minimum)
- Use â€œYes, andâ€¦â€ instead of â€œYes, butâ€¦â€
- Ask â€œWhat else could we build?â€

-----

### Pitfall #2: Analysis Paralysis

**What it looks like:**

- Endless research without decisions
- Perfect understanding before moving forward
- Too many opportunities explored shallowly

**Why it happens:**

- Fear of making wrong choice
- Lack of clear decision criteria
- No timeboxing

**Solution:**

- Set explicit deadlines for each phase
- Use â€œGood enough for now, safe enough to tryâ€
- Make small, reversible decisions
- Use 70% confidence threshold to act

-----

### Pitfall #3: Solo Exploration

**What it looks like:**

- PM does all research and presents conclusions
- Team not involved until build phase
- Lack of shared understanding

**Why it happens:**

- Efficiency mindset (â€œIâ€™ll be faster aloneâ€)
- Protecting team from â€œmessyâ€ research
- Old habits from waterfall

**Solution:**

- Include product trio in all interviews
- Do synthesis sessions together
- Share raw interview recordings
- Co-create solutions as team

-----

### Pitfall #4: Building Without Testing

**What it looks like:**

- Going from idea to full build
- Skipping prototype/test phase
- â€œWeâ€™ll see if users like it after we shipâ€

**Why it happens:**

- Confidence in idea
- Pressure to deliver
- Donâ€™t know how to test cheaply

**Solution:**

- Always ask â€œHow can we test this before building?â€
- Use fake door tests, prototypes, landing pages
- Make â€œtest firstâ€ a team norm
- Celebrate invalidated assumptions as wins

-----

### Pitfall #5: Falling in Love with Opportunities

**What it looks like:**

- Pursuing opportunities that donâ€™t move metrics
- Unable to deprioritize discovered needs
- â€œBut customers said they wanted itâ€

**Why it happens:**

- Empathy for all customer pain
- Not tying opportunities to outcomes
- Difficulty saying no

**Solution:**

- Always connect opportunities to outcome
- Score opportunities by impact on metric
- Remember: You canâ€™t solve everything
- Document deprioritized opportunities for future

-----

## âœ… Exploration Checklist

### Before You Start

- [ ] Outcome clearly defined and measurable
- [ ] Success criteria established
- [ ] Product trio identified (PM, Designer, Engineer)
- [ ] Stakeholder expectations set
- [ ] Time blocked for discovery work
- [ ] Tools ready (interview scheduling, note-taking, synthesis)

### During Problem Space (Discover â†’ Define)

- [ ] 8-15 customer interviews completed
- [ ] Diverse user segments included
- [ ] Synthesis sessions completed with team
- [ ] Patterns and themes identified
- [ ] Problem statement written and validated
- [ ] Success metrics defined
- [ ] HMW questions generated (10-15)
- [ ] Constraints and requirements documented

### During Solution Space (Develop â†’ Deliver)

- [ ] 20+ solution ideas generated
- [ ] Ideas clustered and grouped
- [ ] 3-5 distinct concepts developed
- [ ] Low-fidelity prototypes created
- [ ] Key assumptions documented
- [ ] 5-8 usability tests per concept
- [ ] Concept selected with evidence
- [ ] MVP scoped and prioritized
- [ ] Build-measure-learn plan defined

### After Delivery

- [ ] Impact measured against success criteria
- [ ] Learnings documented
- [ ] Assumptions validated/invalidated recorded
- [ ] Team retrospective completed
- [ ] Process improvements identified
- [ ] Opportunity Solution Tree updated
- [ ] Next exploration topic identified

-----

## ğŸ“š Resources & Tools

### Books

- **â€œContinuous Discovery Habitsâ€** by Teresa Torres (OST)
- **â€œSprintâ€** by Jake Knapp (Design Sprint, similar to Double Diamond)
- **â€œThe Mom Testâ€** by Rob Fitzpatrick (Customer interviews)
- **â€œCompeting Against Luckâ€** by Clayton Christensen (JTBD)
- **â€œJust Enough Researchâ€** by Erika Hall (Research methods)

### Tools

**For Interviews:**

- Calendly (scheduling)
- Zoom (remote interviews)
- Otter.ai (transcription)
- Dovetail (research repository)

**For Synthesis:**

- Miro or Mural (virtual whiteboarding)
- FigJam (collaborative mapping)
- Notion (documentation)
- Airtable (opportunity tracking)

**For Prototyping:**

- Figma (design and prototypes)
- Maze (prototype testing)
- UsabilityHub (quick feedback)
- Loom (walkthrough videos)

**For Tree Management:**

- Miro/Mural (visual OST)
- ProductBoard (opportunity management)
- Notion (documentation)
- Simple Google Sheets (lightweight)

### Templates

**Interview Guides:**

- JTBD interview script
- Problem discovery script
- Usability testing protocol
- Concept testing guide

**Synthesis Templates:**

- Affinity mapping board
- Journey map template
- Opportunity Solution Tree canvas
- How Might We worksheet

-----

## References

- Research Interviews: `../2.3.1-Research-Interviews/README.md`
- Continuous Discovery Habits: `../2.3.2-Continuous-Discovery-Habits/README.md`
- Jobs to Be Done: `../2.3.3-Jobs-To-Be-Done/README.md`
- Opportunity Assessment: `../2.3.4-Opportunity-Assessment/README.md`
- PRD Framework: `../../2.1-Strategy/2.1.4-PRD/README.md`
- Self-Reflection: `../../2.9-Other/2.9.2-Self-Reflection/README.md`